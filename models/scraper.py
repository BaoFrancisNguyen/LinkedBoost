# models/scraper.py - Version mise √† jour avec support Selenium
import asyncio
import time
import logging
from typing import List, Dict, Any
from datetime import datetime, timedelta
from config import Config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ScrapingOrchestrator:
    """Orchestrateur principal pour le scraping avec support Selenium"""
    
    def __init__(self):
        self.scrapers = {}
        self.last_scrape = None
        self.scraping_stats = {
            'total_jobs': 0,
            'last_update': None,
            'errors': [],
            'sources': {},
            'selenium_available': False
        }
        
        # Initialisation des scrapers disponibles
        self.initialize_scrapers()
    
    def initialize_scrapers(self):
        """Initialise les scrapers disponibles avec d√©tection automatique"""
        scrapers_initialized = []
        
        # 1. Tentative d'initialisation du scraper WTTJ avec Selenium
        try:
            from scrapers.wttj_scraper import create_wttj_scraper
            wttj_scraper = create_wttj_scraper()
            self.scrapers['wttj'] = wttj_scraper
            
            # V√©rifier le type de scraper utilis√©
            scraper_type = type(wttj_scraper).__name__
            if 'Fallback' in scraper_type:
                logger.info("‚úÖ Scraper WTTJ initialis√© (mode simulation)")
                self.scraping_stats['selenium_available'] = False
            else:
                logger.info("‚úÖ Scraper WTTJ initialis√© (Selenium activ√©)")
                self.scraping_stats['selenium_available'] = True
            
            scrapers_initialized.append('wttj')
            
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è Import scraper WTTJ √©chou√©: {e}")
            # Fallback vers le scraper simple
            try:
                self.scrapers['wttj'] = SimpleWTTJScraper()
                scrapers_initialized.append('wttj (simple)')
                logger.info("‚úÖ Scraper WTTJ simple activ√©")
            except Exception as e2:
                logger.error(f"‚ùå Impossible d'initialiser le scraper WTTJ: {e2}")
        
        except Exception as e:
            logger.error(f"‚ùå Erreur initialisation scraper WTTJ: {e}")
        
        # 2. Autres scrapers (√† ajouter plus tard)
        # self.initialize_linkedin_scraper()
        # self.initialize_indeed_scraper()
        
        # R√©sum√© de l'initialisation
        if scrapers_initialized:
            logger.info(f"üì° Scrapers disponibles: {scrapers_initialized}")
        else:
            logger.warning("‚ö†Ô∏è Aucun scraper disponible!")
    
    async def run_full_scrape(self, sources: List[str] = None) -> Dict[str, Any]:
        """Lance un scraping complet de toutes les sources"""
        if sources is None:
            sources = list(self.scrapers.keys())
        
        if not sources:
            logger.warning("‚ö†Ô∏è Aucune source de scraping sp√©cifi√©e")
            return {'total_jobs': 0, 'error': 'Aucune source disponible'}
        
        logger.info(f"üöÄ D√©but du scraping pour : {sources}")
        start_time = datetime.now()
        all_jobs = []
        
        for source in sources:
            if source not in self.scrapers:
                logger.warning(f"‚ùå Scraper {source} non disponible")
                self.scraping_stats['errors'].append({
                    'source': source,
                    'error': f'Scraper {source} non configur√©',
                    'timestamp': datetime.now().isoformat()
                })
                continue
                
            try:
                logger.info(f"üì° Scraping {source}...")
                scraper = self.scrapers[source]
                
                # Scraping avec gestion d'erreur sp√©cifique
                jobs = await scraper.scrape_jobs(limit=Config.MAX_JOBS_PER_SCRAPE)
                
                logger.info(f"‚úÖ {len(jobs)} offres r√©cup√©r√©es de {source}")
                all_jobs.extend(jobs)
                
                # Mise √† jour des stats par source
                self.scraping_stats['sources'][source] = {
                    'jobs_count': len(jobs),
                    'last_scrape': datetime.now().isoformat(),
                    'scraper_type': type(scraper).__name__
                }
                
                # D√©lai entre sources pour √©viter la surcharge
                if len(sources) > 1:
                    await asyncio.sleep(Config.REQUEST_DELAY * 2)
                
            except Exception as e:
                error_msg = f"Erreur scraping {source}: {str(e)}"
                logger.error(error_msg)
                self.scraping_stats['errors'].append({
                    'source': source,
                    'error': error_msg,
                    'timestamp': datetime.now().isoformat()
                })
        
        # Traitement et stockage des offres collect√©es
        if all_jobs:
            logger.info(f"üîÑ Traitement de {len(all_jobs)} offres collect√©es...")
            processed_jobs = await self.process_jobs(all_jobs)
            await self.store_jobs(processed_jobs)
        else:
            logger.warning("‚ö†Ô∏è Aucune offre collect√©e")
        
        # Mise √† jour des statistiques finales
        self.scraping_stats.update({
            'total_jobs': len(all_jobs),
            'last_update': datetime.now().isoformat(),
            'duration_seconds': (datetime.now() - start_time).total_seconds(),
            'success_rate': (len(all_jobs) / max(sum(len(self.scrapers) for _ in sources), 1)) * 100
        })
        
        logger.info(f"üéâ Scraping termin√© : {len(all_jobs)} offres en {self.scraping_stats['duration_seconds']:.1f}s")
        return self.scraping_stats
    
    async def process_jobs(self, jobs: List[Dict]) -> List[Dict]:
        """Traite et enrichit les offres d'emploi"""
        logger.info(f"üîÑ Traitement de {len(jobs)} offres...")
        
        processed = []
        for i, job in enumerate(jobs):
            try:
                # Nettoyage et normalisation
                clean_job = {
                    'title': self.clean_text(job.get('title', '')),
                    'company': self.clean_text(job.get('company', '')),
                    'location': self.clean_text(job.get('location', '')),
                    'description': self.clean_text(job.get('description', '')),
                    'contract_type': job.get('contract_type', 'CDI'),
                    'url': job.get('url', ''),
                    'source': job.get('source', 'unknown'),
                    'scraped_at': datetime.now().isoformat(),
                    'hash_id': self.generate_job_hash(job)
                }
                
                # Enrichissement avec analyse de contenu
                if clean_job['description']:
                    clean_job.update({
                        'requirements': self.extract_requirements(clean_job['description']),
                        'salary': self.extract_salary(clean_job['description']),
                        'remote': self.detect_remote(clean_job['description'] + ' ' + clean_job['title']),
                        'experience_level': self.detect_experience_level(clean_job['title'] + ' ' + clean_job['description']),
                        'technologies': self.extract_technologies(clean_job['description']),
                        'benefits': self.extract_benefits(clean_job['description'])
                    })
                
                # Validation des donn√©es essentielles
                if clean_job['title'] and clean_job['company']:
                    processed.append(clean_job)
                    if i < 3:  # Log des premi√®res offres pour debug
                        logger.debug(f"‚úÖ Offre trait√©e: {clean_job['title']} chez {clean_job['company']}")
                else:
                    logger.warning(f"‚ö†Ô∏è Offre ignor√©e (donn√©es manquantes): {job}")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur traitement offre: {e}")
                continue
        
        logger.info(f"‚úÖ {len(processed)} offres trait√©es avec succ√®s")
        return processed
    
    def clean_text(self, text: str) -> str:
        """Nettoie et normalise le texte"""
        import re
        if not text:
            return ""
        
        # Suppression HTML et caract√®res sp√©ciaux
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text[:5000]  # Limite de longueur
    
    def extract_requirements(self, description: str) -> List[str]:
        """Extrait les comp√©tences requises"""
        import re
        
        requirements = []
        patterns = [
            r'(?:comp√©tences?|skills?|requis|required|prerequisites)[:\-\s]+(.*?)(?:\n|\.|;|Profil|Formation)',
            r'(?:ma√Ætrise|exp√©rience|expertise)[:\-\s]+(.*?)(?:\n|\.|;)',
            r'(?:vous ma√Ætrisez|tu ma√Ætrises)[:\-\s]+(.*?)(?:\n|\.|;)',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, description, re.IGNORECASE | re.DOTALL)
            for match in matches:
                # D√©coupage par virgules/points/tirets
                skills = re.split(r'[,;\-‚Ä¢]+', match)
                requirements.extend([skill.strip() for skill in skills if skill.strip() and len(skill.strip()) > 2])
        
        # Filtrage et d√©doublonnage
        unique_requirements = []
        for req in requirements:
            req_clean = req.lower().strip()
            if req_clean not in [r.lower() for r in unique_requirements] and len(req) < 100:
                unique_requirements.append(req.strip())
        
        return unique_requirements[:15]  # Max 15 comp√©tences
    
    def extract_salary(self, description: str) -> Dict[str, Any]:
        """Extrait les informations de salaire avec patterns avanc√©s"""
        import re
        
        salary_patterns = [
            r'(\d+)k?\s*[-‚Äì√†]\s*(\d+)k?\s*‚Ç¨?\s*(?:brut|net)?',
            r'(\d+)\s*k‚Ç¨?\s*(?:brut|net)?(?:\s*(?:par|\/)\s*an)?',
            r'salaire[:\s]*(\d+)[-‚Äì](\d+)\s*k‚Ç¨?',
            r'r√©mun√©ration[:\s]*(\d+)[-‚Äì](\d+)\s*k‚Ç¨?',
            r'package[:\s]*(\d+)[-‚Äì](\d+)\s*k‚Ç¨?',
            r'(\d+)\s*000\s*[-‚Äì]\s*(\d+)\s*000\s*‚Ç¨'
        ]
        
        for pattern in salary_patterns:
            match = re.search(pattern, description, re.IGNORECASE)
            if match:
                groups = match.groups()
                if len(groups) >= 2:
                    min_sal = int(groups[0]) * (1000 if 'k' in match.group(0).lower() or len(groups[0]) <= 3 else 1)
                    max_sal = int(groups[1]) * (1000 if 'k' in match.group(0).lower() or len(groups[1]) <= 3 else 1)
                    
                    return {
                        'found': True,
                        'text': match.group(0),
                        'min': min_sal,
                        'max': max_sal,
                        'currency': 'EUR'
                    }
                else:
                    sal = int(groups[0]) * (1000 if 'k' in match.group(0).lower() or len(groups[0]) <= 3 else 1)
                    return {
                        'found': True,
                        'text': match.group(0),
                        'amount': sal,
                        'currency': 'EUR'
                    }
        
        return {'found': False, 'text': '', 'min': None, 'max': None}
    
    def detect_remote(self, text: str) -> bool:
        """D√©tecte les modalit√©s de travail √† distance"""
        remote_keywords = [
            'remote', 't√©l√©travail', 'home office', 'distanciel', 'hybride',
            '100% remote', 'full remote', 'partiellement remote', 'flex office'
        ]
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in remote_keywords)
    
    def detect_experience_level(self, text: str) -> str:
        """D√©tecte le niveau d'exp√©rience avec patterns avanc√©s"""
        text_lower = text.lower()
        
        # Patterns senior
        senior_patterns = [
            'senior', 'lead', 'principal', 'architect', 'expert', 'confirm√©',
            r'[5-9]\+?\s*ans?', r'[1-9][0-9]\+?\s*ans?', 'exp√©riment√©'
        ]
        
        # Patterns junior
        junior_patterns = [
            'junior', 'd√©butant', 'entry level', 'graduate', 'stage', 'alternance',
            r'0[-\s]?[12]?\s*ans?', 'first job', 'sortie d\'√©cole'
        ]
        
        # Patterns mid-level
        mid_patterns = [
            r'[23456]\s*[-\s]\s*[456789]\s*ans?', 'interm√©diaire', 'mid-level',
            r'[234]\+?\s*ans?'
        ]
        
        # Scoring par cat√©gorie
        import re
        senior_score = sum(1 for pattern in senior_patterns if re.search(pattern, text_lower))
        junior_score = sum(1 for pattern in junior_patterns if re.search(pattern, text_lower))
        mid_score = sum(1 for pattern in mid_patterns if re.search(pattern, text_lower))
        
        if senior_score > max(junior_score, mid_score):
            return 'senior'
        elif junior_score > mid_score:
            return 'junior'
        else:
            return 'mid'
    
    def extract_technologies(self, description: str) -> List[str]:
        """Extrait les technologies avec reconnaissance avanc√©e"""
        # Base de technologies √©tendue
        tech_keywords = {
            # Langages
            'python', 'javascript', 'typescript', 'java', 'go', 'rust', 'php', 'ruby',
            'c++', 'c#', 'scala', 'kotlin', 'swift', 'dart', 'r', 'matlab',
            
            # Frameworks Frontend
            'react', 'vue', 'vue.js', 'angular', 'svelte', 'next.js', 'nuxt.js',
            'jquery', 'bootstrap', 'tailwind', 'material-ui',
            
            # Frameworks Backend
            'django', 'flask', 'fastapi', 'spring', 'laravel', 'node.js', 'express',
            'nestjs', 'rails', 'symfony', 'asp.net',
            
            # Bases de donn√©es
            'postgresql', 'mysql', 'mongodb', 'redis', 'elasticsearch', 'sqlite',
            'oracle', 'cassandra', 'neo4j', 'dynamodb', 'firebase', 'supabase',
            
            # Cloud & DevOps
            'aws', 'azure', 'gcp', 'docker', 'kubernetes', 'terraform', 'ansible',
            'jenkins', 'gitlab', 'github', 'circleci', 'vercel', 'netlify',
            
            # Data & ML
            'pandas', 'numpy', 'tensorflow', 'pytorch', 'scikit-learn', 'spark',
            'hadoop', 'kafka', 'airflow', 'dbt', 'looker', 'tableau', 'powerbi',
            
            # Outils
            'git', 'jira', 'confluence', 'slack', 'notion', 'figma', 'adobe'
        }
        
        found_techs = []
        description_lower = description.lower()
        
        for tech in tech_keywords:
            # Recherche exacte avec d√©limiteurs de mots
            import re
            if re.search(r'\b' + re.escape(tech) + r'\b', description_lower):
                found_techs.append(tech)
        
        return found_techs
    
    def extract_benefits(self, description: str) -> List[str]:
        """Extrait les avantages propos√©s"""
        benefits_keywords = [
            't√©l√©travail', 'remote', 'horaires flexibles', 'flex time',
            'mutuelle', 'assurance sant√©', 'tickets restaurant', 'tr',
            'participation', 'int√©ressement', 'prime', 'bonus',
            'formation', 'certification', 'conf√©rence', 'budget formation',
            'cong√©s', 'rtt', 'vacances', 'cp',
            'sport', 'salle de sport', 'v√©lo', 'transport',
            'afterwork', 'team building', 's√©minaire'
        ]
        
        found_benefits = []
        description_lower = description.lower()
        
        for benefit in benefits_keywords:
            if benefit in description_lower:
                found_benefits.append(benefit)
        
        return found_benefits
    
    def generate_job_hash(self, job: Dict) -> str:
        """G√©n√®re un hash unique pour l'offre"""
        import hashlib
        
        # Utiliser titre + entreprise + URL pour l'unicit√©
        unique_string = f"{job.get('title', '')}{job.get('company', '')}{job.get('url', '')}"
        return hashlib.md5(unique_string.encode()).hexdigest()
    
    async def store_jobs(self, jobs: List[Dict]) -> None:
        """Stocke les offres dans la base de connaissances"""
        logger.info(f"üíæ Stockage de {len(jobs)} offres...")
        
        try:
            # Import conditionnel de la base de connaissances
            from models.knowledge_base import KnowledgeBase
            kb = KnowledgeBase()
            
            stored_count = 0
            for job in jobs:
                try:
                    success = await kb.store_job(job)
                    if success:
                        stored_count += 1
                except Exception as e:
                    logger.error(f"Erreur stockage offre {job.get('title', 'Unknown')}: {e}")
            
            logger.info(f"‚úÖ Stockage termin√© : {stored_count}/{len(jobs)} offres stock√©es")
            
        except ImportError:
            logger.warning("‚ö†Ô∏è Base de connaissances non disponible, stockage ignor√©")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du stockage: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques compl√®tes de scraping"""
        return {
            **self.scraping_stats,
            'available_scrapers': list(self.scrapers.keys()),
            'scraping_enabled': len(self.scrapers) > 0,
            'selenium_status': self.scraping_stats.get('selenium_available', False),
            'last_scrape_time': self.last_scrape.isoformat() if self.last_scrape else None
        }
    
    def get_scraper_info(self, source: str) -> Dict[str, Any]:
        """Retourne les informations sur un scraper sp√©cifique"""
        if source not in self.scrapers:
            return {'available': False, 'error': 'Scraper non configur√©'}
        
        scraper = self.scrapers[source]
        return {
            'available': True,
            'name': getattr(scraper, 'name', source),
            'type': type(scraper).__name__,
            'selenium_based': 'Selenium' in type(scraper).__name__,
            'last_stats': self.scraping_stats.get('sources', {}).get(source, {})
        }


# Scraper simple de fallback (pour compatibilit√©)
class SimpleWTTJScraper:
    """Scraper de fallback avec donn√©es simul√©es r√©alistes"""
    
    def __init__(self):
        self.name = 'Welcome to the Jungle (Simulation)'
    
    async def scrape_jobs(self, limit: int = 50) -> List[Dict[str, Any]]:
        """G√©n√®re des donn√©es simul√©es r√©alistes"""
        logger.info(f"üé≠ G√©n√©ration de {min(limit, 25)} offres simul√©es...")
        
        # Donn√©es d'entreprises fran√ßaises r√©elles
        companies = [
            "Aircall", "BlaBlaCar", "Doctolib", "Criteo", "Deezer",
            "Leboncoin", "Qonto", "Alan", "BackMarket", "Contentsquare",
            "Mirakl", "Algolia", "Datadog", "Dashlane", "Evaneos"
        ]
        
        titles = [
            "D√©veloppeur Full Stack Senior", "Data Scientist", "Product Manager",
            "D√©veloppeur Frontend React", "DevOps Engineer", "UX/UI Designer",
            "Backend Developer Python", "Machine Learning Engineer", "Scrum Master",
            "Lead Developer", "QA Engineer", "Business Analyst"
        ]
        
        locations = [
            "Paris", "Lyon", "Toulouse", "Nantes", "Bordeaux", 
            "Lille", "Marseille", "Remote", "Paris (Hybrid)"
        ]
        
        jobs = []
        for i in range(min(limit, 25)):
            company = companies[i % len(companies)]
            title = titles[i % len(titles)]
            location = locations[i % len(locations)]
            
            job = {
                'title': title,
                'company': company,
                'location': location,
                'description': self.generate_realistic_description(title, company),
                'url': f"https://www.welcometothejungle.com/fr/jobs/sim_{company.lower()}_{i}",
                'source': 'wttj',
                'contract_type': 'CDI',
                'scraped_at': time.time()
            }
            jobs.append(job)
            
            # Petit d√©lai pour simuler le scraping
            await asyncio.sleep(0.1)
        
        logger.info(f"‚úÖ {len(jobs)} offres simul√©es g√©n√©r√©es")
        return jobs
    
    def generate_realistic_description(self, title: str, company: str) -> str:
        """G√©n√®re une description r√©aliste selon le poste"""
        
        tech_stacks = {
            'Frontend': 'React, TypeScript, CSS3, Jest, Webpack',
            'Backend': 'Python, Django, PostgreSQL, Redis, Docker',
            'Full Stack': 'React, Node.js, MongoDB, AWS, Git',
            'Data': 'Python, Pandas, TensorFlow, SQL, Jupyter',
            'DevOps': 'Kubernetes, Terraform, AWS, Jenkins, Monitoring',
            'Mobile': 'React Native, Swift, Kotlin, Firebase'
        }
        
        # D√©terminer la stack selon le titre
        stack = 'Full Stack'  # D√©faut
        for key in tech_stacks:
            if key.lower() in title.lower():
                stack = key
                break
        
        technologies = tech_stacks[stack]
        
        return f"""
{company} recherche un(e) {title} pour rejoindre notre √©quipe en pleine croissance.

üéØ Missions principales :
‚Ä¢ D√©veloppement et maintenance des applications
‚Ä¢ Collaboration avec les √©quipes produit et design  
‚Ä¢ Participation aux d√©cisions techniques et architecturales
‚Ä¢ Code review et mentoring des d√©veloppeurs junior
‚Ä¢ Am√©lioration continue des processus et performances

üîß Stack technique :
{technologies}

üí° Profil recherch√© :
‚Ä¢ 3+ ann√©es d'exp√©rience sur des technologies similaires
‚Ä¢ Ma√Ætrise des m√©thodologies agiles (Scrum/Kanban)
‚Ä¢ Anglais technique courant
‚Ä¢ Esprit d'√©quipe et autonomie
‚Ä¢ Passion pour la tech et l'innovation

üéÅ Ce qu'on propose :
‚Ä¢ T√©l√©travail flexible (2-3 jours/semaine)
‚Ä¢ Formation continue et budget conf√©rences
‚Ä¢ Mutuelle premium et tickets restaurant
‚Ä¢ Stock-options et participation aux b√©n√©fices
‚Ä¢ Environnement stimulant dans une scale-up en croissance

Poste en CDI bas√© √† Paris avec possibilit√© de remote partiel.
R√©mun√©ration selon profil et exp√©rience.
        """.strip()

# Import time pour le fallback
import time
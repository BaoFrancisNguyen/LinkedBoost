# Guide de mise en place - Flask + Ollama avec Mistral

## üìã Pr√©requis

### Logiciels requis
- **Python 3.8+** install√© sur votre syst√®me
- **Ollama** install√© et configur√©
- **Git** (optionnel, pour cloner le projet)

### V√©rification d'Ollama
```bash
# V√©rifier qu'Ollama est install√©
ollama --version

# D√©marrer Ollama (si pas d√©j√† fait)
ollama serve

# T√©l√©charger Mistral
ollama pull mistral:latest

# V√©rifier que Mistral est disponible
ollama list
```

## üöÄ Installation rapide

### 1. Cr√©ation de la structure du projet
```bash
mkdir flask-ollama-app
cd flask-ollama-app

# Cr√©ation des dossiers
mkdir -p app/{routes,services,templates,static/{css,js}}
```

### 2. Environnement virtuel Python
```bash
# Cr√©ation de l'environnement virtuel
python -m venv venv

# Activation (Linux/Mac)
source venv/bin/activate

# Activation (Windows)
venv\Scripts\activate
```

### 3. Installation des d√©pendances
```bash
# Cr√©er le fichier requirements.txt avec le contenu fourni
pip install -r requirements.txt
```

### 4. Fichiers √† cr√©er

Cr√©ez les fichiers suivants avec le contenu fourni dans les artefacts :

**Structure finale :**
```
flask-ollama-app/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api.py
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ollama_service.py
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat.html
‚îÇ   ‚îî‚îÄ‚îÄ static/
‚îÇ       ‚îú‚îÄ‚îÄ css/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ style.css
‚îÇ       ‚îî‚îÄ‚îÄ js/
‚îÇ           ‚îî‚îÄ‚îÄ app.js
‚îú‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ run.py
‚îú‚îÄ‚îÄ .env
‚îî‚îÄ‚îÄ README.md
```

### 5. Configuration
```bash
# Cr√©er le fichier .env
echo "FLASK_DEBUG=True
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral:latest
SECRET_KEY=your-secret-key-change-in-production" > .env
```

## üéØ Lancement de l'application

### M√©thode 1 : D√©marrage manuel
```bash
# 1. D√©marrer Ollama (dans un terminal s√©par√©)
ollama serve

# 2. Activer l'environnement virtuel
source venv/bin/activate

# 3. D√©marrer Flask
python run.py
```

### M√©thode 2 : Script automatis√©
```bash
# Rendre le script ex√©cutable
chmod +x start.sh

# Lancer l'application
./start.sh
```

### M√©thode 3 : Docker (pour d√©ploiement)
```bash
# Construction et d√©marrage avec Docker Compose
docker-compose up -d

# V√©rification des logs
docker-compose logs -f
```

## üåê Acc√®s √† l'application

Une fois l'application d√©marr√©e, acc√©dez √† :
- **Page d'accueil** : http://localhost:5000
- **Interface de chat** : http://localhost:5000/chat
- **API de statut** : http://localhost:5000/status

## üîß Fonctionnalit√©s principales

### Interface Web
- **Chat en temps r√©el** avec Mistral
- **Streaming des r√©ponses** pour une exp√©rience fluide
- **Configuration des param√®tres** (temp√©rature, tokens max)
- **Export des conversations** en Markdown
- **Interface responsive** adapt√©e mobile

### API REST
- `POST /api/generate` - G√©n√©ration de r√©ponse
- `POST /api/stream` - G√©n√©ration avec streaming
- `GET /api/models` - Liste des mod√®les disponibles
- `GET /status` - Statut de l'application

### Exemple d'utilisation de l'API
```bash
# Test de g√©n√©ration simple
curl -X POST http://localhost:5000/api/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Bonjour, comment allez-vous?"}'

# Test avec options personnalis√©es
curl -X POST http://localhost:5000/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Expliquez-moi les bases de Python",
    "options": {
      "temperature": 0.7,
      "num_predict": 200
    }
  }'
```

## üõ† Personnalisation

### Modification du mod√®le
```python
# Dans config.py
OLLAMA_MODEL = 'llama2:latest'  # Ou tout autre mod√®le
```

### Ajout de nouveaux mod√®les
```bash
# T√©l√©charger d'autres mod√®les
ollama pull llama2:latest
ollama pull codellama:latest
ollama pull phi:latest
```

### Configuration avanc√©e
```python
# Dans app/services/ollama_service.py
def generate_response(self, prompt, **kwargs):
    payload = {
        "model": self.model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": kwargs.get('temperature', 0.7),
            "top_p": kwargs.get('top_p', 0.9),
            "top_k": kwargs.get('top_k', 40),
            "num_predict": kwargs.get('num_predict', 150),
            "repeat_penalty": kwargs.get('repeat_penalty', 1.1)
        }
    }
```

## üìä Monitoring et logs

### V√©rification du statut
```bash
# Statut de l'application
curl http://localhost:5000/status

# Logs de Flask
tail -f app.log  # Si configur√©

# Logs d'Ollama
ollama logs
```

### M√©triques disponibles
- Statut de connexion √† Ollama
- Mod√®les disponibles
- Temps de r√©ponse
- Erreurs de communication

## üêõ D√©pannage

### Probl√®mes courants

**1. Ollama non disponible**
```bash
# V√©rifier qu'Ollama fonctionne
curl http://localhost:11434/api/tags

# Red√©marrer Ollama si n√©cessaire
pkill ollama
ollama serve
```

**2. Mod√®le non trouv√©**
```bash
# V√©rifier les mod√®les install√©s
ollama list

# T√©l√©charger Mistral si absent
ollama pull mistral:latest
```

**3. Erreurs de d√©pendances Python**
```bash
# R√©installer les d√©pendances
pip install --upgrade -r requirements.txt
```

**4. Port d√©j√† utilis√©**
```bash
# Changer le port dans run.py
app.run(debug=True, host='0.0.0.0', port=5001)
```

### Logs de d√©bogage
```python
# Activer les logs d√©taill√©s dans config.py
import logging
logging.basicConfig(level=logging.DEBUG)
```

## üöÄ D√©ploiement en production

### Configuration de production
```python
# config.py - Classe de production
class ProductionConfig(Config):
    DEBUG = False
    SECRET_KEY = os.environ.get('SECRET_KEY')
    OLLAMA_BASE_URL = os.environ.get('OLLAMA_BASE_URL', 'http://ollama:11434')
```

### Variables d'environnement
```bash
export FLASK_ENV=production
export SECRET_KEY=your-very-secure-secret-key
export OLLAMA_BASE_URL=http://your-ollama-server:11434
```

### Serveur WSGI (Gunicorn)
```bash
# Installation
pip install gunicorn

# D√©marrage
gunicorn -w 4 -b 0.0.0.0:5000 run:app
```

### Nginx (reverse proxy)
```nginx
server {
    listen 80;
    server_name your-domain.com;
    
    location / {
        proxy_pass http://127.0.0.1:5000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
    
    location /api/stream {
        proxy_pass http://127.0.0.1:5000;
        proxy_buffering off;
        proxy_cache off;
    }
}
```

## üîê S√©curit√©

### Bonnes pratiques
- Changez la `SECRET_KEY` en production
- Utilisez HTTPS en production
- Limitez l'acc√®s √† l'API si n√©cessaire
- Surveillez les logs pour d√©tecter les abus

### Limitation de d√©bit (optionnel)
```python
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

limiter = Limiter(
    app,
    key_func=get_remote_address,
    default_limits=["200 per day", "50 per hour"]
)

@limiter.limit("10 per minute")
@api_bp.route('/generate', methods=['POST'])
def generate():
    # ...
```

## üìà Am√©liorations possibles

### Fonctionnalit√©s avanc√©es
- **Authentification utilisateur**
- **Sauvegarde des conversations** en base de donn√©es
- **Support multi-mod√®les** en temps r√©el
- **Interface d'administration**
- **Statistiques d'utilisation**
- **API de gestion des mod√®les**

### Optimisations
- **Cache des r√©ponses** fr√©quentes
- **Pool de connexions** √† Ollama
- **Compression des r√©ponses**
- **CDN pour les assets statiques**

## ü§ù Contribution

### Structure du code
- `app/routes/` - Contr√¥leurs Flask
- `app/services/` - Logique m√©tier
- `app/templates/` - Templates HTML
- `app/static/` - CSS, JS, images

### Tests
```bash
# Installer les d√©pendances de test
pip install pytest pytest-flask

# Lancer les tests
python -m pytest tests/
```

## üìö Ressources

- [Documentation Ollama](https://github.com/jmorganca/ollama)
- [Documentation Flask](https://flask.palletsprojects.com/)
- [Mod√®les disponibles](https://ollama.ai/library)
- [Guide Mistral](https://docs.mistral.ai/)

---

**üéâ Votre application Flask + Ollama est maintenant pr√™te !**

D√©marrez avec `python run.py` et acc√©dez √† http://localhost:5000